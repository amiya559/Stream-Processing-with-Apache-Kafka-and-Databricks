{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9706c138-efb6-4052-ae7c-1c6baf5051ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "confluentClusterName = \"kafka-with-databricks\"\n",
    "confluentBootstrapServers = \"pkc-lgwgm.eastus2.azure.confluent.cloud:9092\"\n",
    "confluentTopicName = \"orders\"\n",
    "confluentApiKey = \"EFT7XS6PO2554TVM\"\n",
    "confluentSecret = \"cflt4+/pWRCJXakCBUujthgjiCk+/K6lNvoN22um6DEtZu2a6/KwtsYweSprnjtw\"\n",
    "schemaRegistryUrl = \"https://psrc-8qyy0.eastus2.azure.confluent.cloud\"\n",
    "confluentRegistryApiKey = \"PJ2IJQFJ35AVY2BJ\"\n",
    "confluentRegistrySecret = \"cflt+Zh4iEzxbfy7aSnTDYVOegH3bqp8ARedoMUPbmcfkb0gYLoJj6GuEVe3Ntig\"\n",
    "deltaTablePath = \"/Volumes/demo_catalog_for_streaming_data/data-from-kafka/raw_data/orders\"\n",
    "checkpointPath = \"/Volumes/demo_catalog_for_streaming_data/data-from-kafka/raw_data/checkpoint/orders\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92d59bd2-b3aa-4e72-9e03-0126a8b5db88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from confluent_kafka.schema_registry import SchemaRegistryClient\n",
    "import ssl\n",
    "\n",
    "schema_registry_conf = {\n",
    "    \"url\": schemaRegistryUrl,\n",
    "    \"basic.auth.user.info\": \"{}:{}\".format(\n",
    "        confluentRegistryApiKey, confluentRegistrySecret\n",
    "    ),\n",
    "}\n",
    "\n",
    "schema_registry_client = SchemaRegistryClient(schema_registry_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6900cc52-6d17-4f22-aee0-a45945ed7b1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as fn\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "binary_to_string = fn.udf(\n",
    "    lambda x: str(int.from_bytes(x, byteorder=\"big\")), StringType()\n",
    ")\n",
    "streamTestDf = (\n",
    "    spark.readStream.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", confluentBootstrapServers)\n",
    "    .option(\"kafka.security.protocol\", \"SASL_SSL\")\n",
    "    .option(\n",
    "        \"kafka.sasl.jaas.config\",\n",
    "        \"kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username='{}' password='{}';\".format(\n",
    "            confluentApiKey, confluentSecret\n",
    "        ),\n",
    "    )\n",
    "    .option(\"kafka.ssl.endpoint.identification.algorithm\", \"https\")\n",
    "    .option(\"kafka.sasl.mechanism\", \"PLAIN\")\n",
    "    .option(\"subscribe\", confluentTopicName)\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .option(\"failOnDataLoss\", \"false\")\n",
    "    .load()\n",
    "    .withColumn(\"key\", fn.col(\"key\").cast(StringType()))\n",
    "    .withColumn(\"fixedValue\", fn.expr(\"substring(value, 6, length(value)-5)\"))\n",
    "    .withColumn(\"valueSchemaId\", binary_to_string(fn.expr(\"substring(value, 2, 4)\")))\n",
    "    .select(\n",
    "        \"topic\",\n",
    "        \"partition\",\n",
    "        \"offset\",\n",
    "        \"timestamp\",\n",
    "        \"timestampType\",\n",
    "        \"key\",\n",
    "        \"valueSchemaId\",\n",
    "        \"fixedValue\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2968d61c-eb2b-4ac0-92b3-d861b075eabb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "streamTestDf.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"checkpointLocation\", \"/Volumes/demo_catalog_for_streaming_data/data-from-kafka/raw_data\") \\\n",
    "    .option(\"path\", \"/Volumes/demo_catalog_for_streaming_data/data-from-kafka/raw_data\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e5b0f43-8e0c-44f8-aa02-d6ff75b0b21a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as fn\n",
    "from pyspark.sql.avro.functions import from_avro\n",
    "\n",
    "\n",
    "def parseAvroDataWithSchemaId(df, ephoch_id):\n",
    "    cachedDf = df.cache()\n",
    "    fromAvroOptions = {\"mode\": \"FAILFAST\"}\n",
    "\n",
    "    def getSchema(id):\n",
    "        return str(schema_registry_client.get_schema(id).schema_str)\n",
    "\n",
    "    distinctValueSchemaIdDF = cachedDf.select(\n",
    "        fn.col(\"valueSchemaId\").cast(\"integer\")\n",
    "    ).distinct()\n",
    "    for valueRow in distinctValueSchemaIdDF.collect():\n",
    "        currentValueSchemaId = sc.broadcast(valueRow.valueSchemaId)\n",
    "        currentValueSchema = sc.broadcast(getSchema(currentValueSchemaId.value))\n",
    "        filterValueDF = cachedDf.filter(\n",
    "            fn.col(\"valueSchemaId\") == currentValueSchemaId.value\n",
    "        )\n",
    "        filterValueDF.select(\n",
    "            \"topic\",\n",
    "            \"partition\",\n",
    "            \"offset\",\n",
    "            \"timestamp\",\n",
    "            \"timestampType\",\n",
    "            \"key\",\n",
    "            from_avro(\"fixedValue\", currentValueSchema.value, fromAvroOptions).alias(\n",
    "                \"parsedValue\"\n",
    "            ),\n",
    "        ).write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").save(\n",
    "            deltaTablePath\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3feebaa8-5c79-49d3-a683-3a339e9e4c35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def parseAvroDataWithSchemaId(\n",
    "    batch_df,\n",
    "    batch_id\n",
    "):\n",
    "    # Access Spark session only via batch_df.sparkSession\n",
    "    # All processing should use batch_df and batch_id\n",
    "    pass\n",
    "\n",
    "streamTestDf.writeStream \\\n",
    "    .option(\"checkpointLocation\", checkpointPath) \\\n",
    "    .foreachBatch(parseAvroDataWithSchemaId) \\\n",
    "    .queryName(\"clickStreamTestFromConfluent\") \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6e808ba-f359-4ee4-8408-c83d625a6144",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "deltaTestDf = spark.read.format(\"delta\").load(deltaTablePath)\n",
    "display(deltaTestDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ab10d79-7c5f-4545-b49e-f811b27ae3c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "deltaTestDf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14d7a2c1-5836-4e4d-af92-0c5a10af4cec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "deltaTestDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0ca177b-e44d-4fdd-ba9b-f2f4387713dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Assuming your DataFrame is named 'df'\n",
    "new_df = deltaTestDf.select(\n",
    "    col('topic'),\n",
    "    col('partition'),\n",
    "    col('offset'),\n",
    "    col('timestamp'),\n",
    "    col('timestampType'),\n",
    "    col('key'),\n",
    "    col('parsedValue.ordertime').alias('ordertime'),\n",
    "    col('parsedValue.orderid').alias('orderid'),\n",
    "    col('parsedValue.itemid').alias('itemid'),\n",
    "    col('parsedValue.orderunits').alias('orderunits'),\n",
    "    col('parsedValue.address.city').alias('city'),\n",
    "    col('parsedValue.address.state').alias('state'),\n",
    "    col('parsedValue.address.zipcode').alias('zipcode')\n",
    ")\n",
    "\n",
    "# Now 'new_df' contains exploded columns from 'parsedValue'\n",
    "new_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d65d3b1c-e49e-4eb6-a587-b26843f8a6bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": "HIGH"
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "dependencies": [
     "confluent-kafka[avro,json,protobuf]>=1.4.2"
    ],
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "stream-kafka_cluster",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
